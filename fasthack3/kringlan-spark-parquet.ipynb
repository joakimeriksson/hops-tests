{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58eab6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>94</td><td>application_1638826983412_0036</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hops3-master-upgrade.internal.cloudapp.net:8089/proxy/application_1638826983412_0036/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hops3-worker-1.internal.cloudapp.net:8044/node/containerlogs/container_e10_1638826983412_0036_01_000001/Kringlan__niclasfi\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Store Kafka JSON data to a parquet file on HDFS\n",
    "#\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, BooleanType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, TimestampType, LongType\n",
    "from pyspark.sql import functions\n",
    "from hops import kafka\n",
    "from hops import tls\n",
    "from hops import hdfs\n",
    "\n",
    "# Setup the Schema for storing the data\n",
    "schema = StructType([\n",
    "    StructField(\"observation\", StructType([\n",
    "        StructField(\"value\", StringType(), True),\n",
    "        StructField(\"quantityKind\", StringType(), True),\n",
    "        StructField(\"sensorId\", StringType(), True),\n",
    "        StructField(\"observationTime\", StringType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"sensor\", StringType(), True)])\n",
    "\n",
    "# Setup Topic namnet\n",
    "TOPIC_NAME = \"kringlan-topic-1\"\n",
    "config = kafka.get_kafka_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422a2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Spark Kafka stream\n",
    "dfk = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka.get_broker_endpoints()) \\\n",
    "  .option(\"kafka.security.protocol\",kafka.get_security_protocol()) \\\n",
    "  .option(\"kafka.ssl.truststore.location\", tls.get_trust_store()) \\\n",
    "  .option(\"kafka.ssl.truststore.password\", tls.get_key_store_pwd()) \\\n",
    "  .option(\"kafka.ssl.keystore.location\", tls.get_key_store()) \\\n",
    "  .option(\"kafka.ssl.keystore.password\", tls.get_key_store_pwd()) \\\n",
    "  .option(\"kafka.ssl.key.password\", tls.get_trust_store_pwd()) \\\n",
    "  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"\") \\\n",
    "  .option(\"subscribe\", TOPIC_NAME) \\\n",
    "  .load()\n",
    "\n",
    "# If in need of reading from start!\n",
    "#  .option(\"startingOffsets\", \"earliest\") \\\n",
    "\n",
    "# Stream to Parquet fil\n",
    "PARQ_PATH  = \"/Projects/\" + hdfs.project_name() + \"/Jupyter/Data/eventhub-stream.parquet\"\n",
    "CHECK_PATH = \"/Projects/\" + hdfs.project_name() + \"/Jupyter/Data/eventhub-stream-checkpoint/\"\n",
    "\n",
    "df_output = dfk \\\n",
    "        .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .select(functions.from_json(\"value\", schema=schema).alias(\"data\"))\n",
    "\n",
    "# parquet sink example - will store just once!\n",
    "targetParquetHDFS = df_output \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", PARQ_PATH) \\\n",
    "    .option(\"checkpointLocation\", CHECK_PATH) \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "#    .trigger(processingTime=\"120 seconds\") \\\n",
    "\n",
    "targetParquetHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402fbeda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}